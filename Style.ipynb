{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3jNOs0GKDCJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXes28liMXc9"
      },
      "outputs": [],
      "source": [
        "path_to_file = \"/content/hafez.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfC4vE_9KDfP",
        "outputId": "a020e861-4e32-4e12-d859-509993590364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', \n",
        "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn11s63pKLUv",
        "outputId": "2dcf4707-8278-43bd-c011-ff3e28fbb7d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of characters in the corpus is: 1115394\n",
            "The first 100 characters of the corpus are as follows:\n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read()\n",
        "text = text.decode(encoding='utf-8')\n",
        "print ('Total number of characters in the corpus is:', len(text))\n",
        "print('The first 100 characters of the corpus are as follows:\\n', text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7ikQzFvKZI0",
        "outputId": "d3153823-eb53-425a-9ae1-e3eed8f13550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of unique characters in the corpus is 65\n",
            "A slice of the unique characters set:\n",
            " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the corpus\n",
        "vocab = sorted(set(text))\n",
        "print ('The number of unique characters in the corpus is', len(vocab))\n",
        "print('A slice of the unique characters set:\\n', vocab[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydbTbmbzKkeQ"
      },
      "outputs": [],
      "source": [
        "# Create a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "# Make a copy of the unique set elements in NumPy array format for later use in the decoding the predictions\n",
        "idx2char = np.array(vocab)\n",
        "# Vectorize the text with a for loop\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LRnJCjZLDs2"
      },
      "outputs": [],
      "source": [
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) \n",
        "# for i in char_dataset.take(5): \n",
        "#   print(i.numpy())\n",
        "seq_length = 100 # The max. length for single input\n",
        "# examples_per_epoch = len(text)//(seq_length+1) # double-slash for “floor” division\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) \n",
        "# for item in sequences.take(5): \n",
        "#   print(repr(''.join(idx2char[item.numpy()])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH9VA-xxLY0c"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8tv6q_hL2MS",
        "outputId": "12eeaa57-063e-469b-bb8d-d47788a23dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "BUFFER_SIZE = 10000 # TF shuffles the data only within buffers\n",
        "\n",
        "BATCH_SIZE = 64 # Batch size\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRXwXAVUMiJO"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl1MxSdSM8v2"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpochpVWND52",
        "outputId": "ebb2feeb-db66-45e5-a0d0-953f79863bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab), # no. of unique characters\n",
        "    embedding_dim=embedding_dim, # 256\n",
        "    rnn_units=rnn_units, # 1024\n",
        "    batch_size=BATCH_SIZE)  # 64 for the traning\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0V6tnAFNTiH"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "# print(\"Prediction shape: \", example_batch_predictions.shape, \" (batch_size, sequence_length, vocab_size)\")\n",
        "# print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y83RTsumNcxt"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dVavpDFNh8P",
        "outputId": "9c461e06-0b3d-490e-f2bd-7915d40a4c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 17s 53ms/step - loss: 2.6728\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.9729\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.7018\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.5498\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.4609\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.3987\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.3522\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.3138\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2791\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2449\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2134\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1815\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.1488\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.1158\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0818\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0455\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0095\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.9740\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 11s 59ms/step - loss: 0.9385\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9048\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.8717\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.8422\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.8144\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.7902\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.7685\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7481\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7325\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7168\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7046\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.6936\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset, \n",
        "                    epochs=EPOCHS, \n",
        "                    callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "7mD9E5QYNknU",
        "outputId": "345ade2e-04af-42c4-e4e1-a19ae8d15fdd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIClIh5-QCSG",
        "outputId": "cec36dda-a7d2-469c-c700-2bfe23d3e8cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 256)            12544     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 49)             50225     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,001,073\n",
            "Trainable params: 4,001,073\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRS2_aDJQFiS"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, num_generate, temperature, start_string):\n",
        "  input_eval = [char2idx[s] for s in start_string] # string to numbers (vectorizing)\n",
        "  input_eval = tf.expand_dims(input_eval, 0) # dimension expansion\n",
        "  text_generated = [] # Empty string to store our results\n",
        "  model.reset_states() # Clears the hidden states in the RNN\n",
        "\n",
        "  for i in range(num_generate): #Run a loop for number of characters to generate\n",
        "    predictions = model(input_eval) # prediction for single character\n",
        "    predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    # higher temperature increases the probability of selecting a less likely character\n",
        "    # lower --> more predictable\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # The predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    # So the model makes the next prediction based on the previous character\n",
        "    input_eval = tf.expand_dims([predicted_id], 0) \n",
        "    # Also devectorize the number and add to the generated text\n",
        "    text_generated.append(idx2char[predicted_id]) \n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74IBZ3XAQPe2",
        "outputId": "2d699001-80f6-4221-e26b-89c738389f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "عشق آينه کرد\n",
            "کنار آب روی شود ز بهمان غزل خوان بودی\n",
            "\n",
            "\n",
            "غزل    ۴۳۵\n",
            "\n",
            "ببين که رقص کنان می‌شکد دل من\n",
            "که گشت هوای ميکده يک سال من آگاه که خزينه\n",
            "\n",
            "بدوش معشوق وجودنا اندر آن جا فکن\n",
            "به اميد سيه اميدم که صبت مقرم ندود زهی سر ببين\n",
            "\n",
            "اگر چه مردم تو شکه صارم ور نی\n",
            "به هيچ بابند حريفان نه کنون که خطر زمانه\n",
            "\n",
            "من مراد آنم که چه گذر از هر کو چشم بر نم\n",
            "آزم جز لطف و خنيار می‌آورد\n",
            "دلبر برفت حافظ از اين خرقه پشا نيست\n",
            "دولت‌ها شد کنم چون لطف خدادان چه کنم\n",
            "\n",
            "برخانه بدنيم و مکن نرود از کوی\n",
            "تا در ميان ميان و دريغ مدار\n",
            "\n",
            "و ادب باشمم و يار بهاران است که خوش باد\n",
            "که ريز است غم هجران تو بست\n",
            "يع بر کوی تو از هر فتنه‌اش آن جا\n",
            "منزل نيست فرح بدان می‌خورم\n",
            "\n",
            "صوار باد گل چو يار حور و پر از عيب کند\n",
            "که الاقی من و او نشان فدا می‌کردم\n",
            "\n",
            "کشته غمزه تو امشاد در دلايت دارم\n",
            "که ز تيره آبروی خود افکنی به کس آور جلوه گرد نبخشد\n",
            "دل شود از لعل نوشينان نخواهد شد\n",
            "\n",
            "مروم نه دل برد و کمرخشاند\n",
            "ترسم طلب بگذشت در ره عقل می‌دهد\n",
            "يک نفس بلبل چو از کرم صارا آن خاک پيشان\n",
            "باشد که خاک معتاقه دلکش عشاق سوخت\n",
            "جنه‌ای خرج آورد فرياد است خدا را\n",
            "که به روی شاهودن\n",
            "که گوش که\n"
          ]
        }
      ],
      "source": [
        "generated_text = generate_text(\n",
        "                    model, \n",
        "                    num_generate=1000, \n",
        "                    temperature=1, \n",
        "                    start_string=u\"عشق\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCfe6jh7QSiZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}